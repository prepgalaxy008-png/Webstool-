import os
import asyncio
import logging
import re
from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from googlesearch import search
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from aiohttp import web

# 1. Setup
logging.basicConfig(level=logging.INFO)
API_TOKEN = os.getenv('BOT_TOKEN')
ADMIN_ID = 7020885934 # ‡§Ü‡§™‡§ï‡•Ä ID ‡§∏‡•á‡§ü ‡§π‡•à

bot = Bot(token=API_TOKEN)
dp = Dispatcher()
stats = {"total_users": set(), "checks_done": 0}

# --- 2. Advanced Cleaner (Wikipedia Fix) ---
def ultra_clean(text):
    # 1. Wikipedia ‡§ï‡•á [1], [26][27] ‡§ú‡•à‡§∏‡•á ‡§®‡§Ç‡§¨‡§∞‡•ç‡§∏ ‡§ï‡•ã ‡§π‡§ü‡§æ‡§§‡§æ ‡§π‡•à
    text = re.sub(r'\[\d+\]', '', text)
    # 2. ‡§∏‡•ç‡§™‡•á‡§∂‡§≤ ‡§ï‡•à‡§∞‡•á‡§ï‡•ç‡§ü‡§∞‡•ç‡§∏ ‡§î‡§∞ ‡§è‡§ï‡•ç‡§∏‡•ç‡§ü‡•ç‡§∞‡§æ ‡§∏‡•ç‡§™‡•á‡§∏ ‡§π‡§ü‡§æ‡§§‡§æ ‡§π‡•à
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# --- 3. Render Server ---
async def start_web_server():
    app = web.Application()
    app.router.add_get('/', lambda r: web.Response(text="Bot is Live!"))
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', int(os.environ.get("PORT", 8080)))
    await site.start()

# --- 4. Logic & Handlers ---
@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    stats["total_users"].add(message.from_user.id)
    await message.answer("‚úÖ **Advanced Bot Active!**\nWikipedia ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§≠‡•á‡§ú‡§ï‡§∞ ‡§ü‡•á‡§∏‡•ç‡§ü ‡§ï‡§∞‡•á‡§Ç‡•§")

@dp.message(F.text)
async def handle_text(msg: types.Message):
    if msg.text.startswith('/'): return
    
    stats["checks_done"] += 1
    stats["total_users"].add(msg.from_user.id)
    
    # ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ï‡•ã ‡§™‡•Ç‡§∞‡•Ä ‡§§‡§∞‡§π ‡§∏‡§æ‡•û ‡§ï‡§∞‡§®‡§æ
    cleaned_text = ultra_clean(msg.text)
    
    m = await msg.answer("üåê ‡§á‡§Ç‡§ü‡§∞‡§®‡•á‡§ü ‡§™‡§∞ ‡§∏‡§ü‡•Ä‡§ï ‡§ñ‡•ã‡§ú ‡§ú‡§æ‡§∞‡•Ä ‡§π‡•à...")
    
    # ‡§∏‡§¨‡§∏‡•á ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§µ‡§æ‡§ï‡•ç‡§Ø (Sentence) ‡§ö‡•Å‡§®‡§®‡§æ
    sentences = [s for s in cleaned_text.split('.') if len(s) > 30]
    query = sentences[0] if sentences else cleaned_text[:80]

    links = []
    try:
        # Exact Match ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§¨‡§≤ ‡§ï‡•ã‡§ü‡•ç‡§∏ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó
        for url in search(f'"{query[:100]}"', num_results=3):
            links.append(url)
    except Exception as e:
        logging.error(f"Search Error: {e}")

    if links:
        report = "üö® **Plagiarism Detected!**\n\n‡§Ø‡§π‡§æ‡§Å ‡§Æ‡•à‡§ö ‡§Æ‡§ø‡§≤‡§æ ‡§π‡•à:\n" + "\n".join([f"üîó {l}" for l in links])
        await m.edit_text(report, disable_web_page_preview=True)
    else:
        # ‡§Ö‡§ó‡§∞ ‡§ï‡•ã‡§ü‡•ç‡§∏ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡•à‡§ö ‡§® ‡§Æ‡§ø‡§≤‡•á ‡§§‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∏‡§∞‡•ç‡§ö
        try:
            for url in search(query[:100], num_results=2):
                links.append(url)
        except: pass
        
        if links:
            await m.edit_text("üö® **Potential Match Found:**\n" + "\n".join(links), disable_web_page_preview=True)
        else:
            await m.edit_text("‚úÖ ‡§Ø‡§π ‡§ï‡§Ç‡§ü‡•á‡§Ç‡§ü ‡§á‡§Ç‡§ü‡§∞‡§®‡•á‡§ü ‡§™‡§∞ ‡§ì‡§∞‡§ø‡§ú‡§ø‡§®‡§≤ ‡§≤‡§ó ‡§∞‡§π‡§æ ‡§π‡•à!")

async def main():
    await start_web_server()
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
